
import os, sys, time, pickle, tempfile
import sys
import numpy as np
import pandas as pd
import math, random, itertools
from sklearn.cluster import KMeans
from joblib import Parallel, delayed
from randomguess import maketrajall,extract_traj

random.seed(1234567890)
np.random.seed(1234567890)
LOG_SMALL = -10
LOG_ZERO = -1000
ranksvm_dir = '/home/marlboro/Downloads/liblinear-ranksvm-2.1'
BIN_CLUSTER = 5  # discritization parameter
RANKSVM_COST = 10

file = 'geolife_traj.txt'
traj_all,trajid_set_all = maketrajall(file)

fpoi = ''
poi_all = pd.read_csv(fpoi)
poi_all.set_index('poiID', inplace=True)
poi_all.head()

traj_dict = dict()
for trajid in trajid_set_all:
    traj = extract_traj(trajid, traj_all)
    assert(trajid not in traj_dict)
    traj_dict[trajid] = traj

QUERY_ID_DICT = dict()  # (start, end, length) --> qid
keys = [(traj_dict[x][0], traj_dict[x][-1], len(traj_dict[x])) \
        for x in sorted(traj_dict.keys()) if len(traj_dict[x]) > 2]
cnt = 0
for key in keys:
    if key not in QUERY_ID_DICT:   # (start, end, length) --> qid
        QUERY_ID_DICT[key] = cnt
        cnt += 1


# python wrapper of rankSVM
class RankSVM:
    def __init__(self, bin_dir, useLinear=True, debug=False):
        dir_ = !echo $bin_dir  # deal with environmental variables in path
        assert (os.path.exists(dir_[0]))
        self.bin_dir = dir_[0]

        self.bin_train = 'svm-train'
        self.bin_predict = 'svm-predict'
        if useLinear:
            self.bin_train = 'train'
            self.bin_predict = 'predict'

        assert (isinstance(debug, bool))
        self.debug = debug

        # create named tmp files for model and feature scaling parameters
        self.fmodel = None
        self.fscale = None
        with tempfile.NamedTemporaryFile(delete=False) as fd:
            self.fmodel = fd.name
        with tempfile.NamedTemporaryFile(delete=False) as fd:
            self.fscale = fd.name

        if self.debug:
            print('model file:', self.fmodel)
            print('feature scaling parameter file:', self.fscale)

    def __del__(self):
        # remove tmp files
        if self.debug == False:
            if self.fmodel is not None and os.path.exists(self.fmodel):
                os.unlink(self.fmodel)
            if self.fscale is not None and os.path.exists(self.fscale):
                os.unlink(self.fscale)

    def train(self, train_df, cost=1):
        # cost is parameter C in SVM
        # write train data to file
        ftrain = None
        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd:
            ftrain = fd.name
            datastr = gen_data_str(train_df)
            fd.write(datastr)

        # feature scaling
        ftrain_scaled = None
        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd:
            ftrain_scaled = fd.name
        result = !$self.bin_dir / svm - scale - s $self.fscale $ftrain > $ftrain_scaled

        if self.debug:
            print('cost:', cost)
            print('train data file:', ftrain)
            print('feature scaled train data file:', ftrain_scaled)

        # train rank svm and generate model file, if the model file exists, rewrite it
        result = !$self.bin_dir /$self.bin_train - c $cost $ftrain_scaled $self.fmodel
        if self.debug:
            print('Training finished.')
            for i in range(len(result)): print(result[i])

        # remove train data file
        if self.debug == False:
            os.unlink(ftrain)
            os.unlink(ftrain_scaled)

    def predict(self, test_df):
        # predict ranking scores for the given feature matrix
        if self.fmodel is None or not os.path.exists(self.fmodel):
            print('Model should be trained before prediction')
            return

        # write test data to file
        ftest = None
        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd:
            ftest = fd.name
            datastr = gen_data_str(test_df)
            fd.write(datastr)

        # feature scaling
        ftest_scaled = None
        with tempfile.NamedTemporaryFile(delete=False) as fd:
            ftest_scaled = fd.name
        result = !$self.bin_dir / svm - scale - r $self.fscale $ftest > $ftest_scaled

        # generate prediction file
        fpredict = None
        with tempfile.NamedTemporaryFile(delete=False) as fd:
            fpredict = fd.name

        if self.debug:
            print('test data file:', ftest)
            print('feature scaled test data file:', ftest_scaled)
            print('predict result file:', fpredict)

        # predict using trained model and write prediction to file
        result = !$self.bin_dir /$self.bin_predict $ftest_scaled $self.fmodel $fpredict
        if self.debug:
            print('Predict result: %-30s  %s' % (result[0], result[1]))

        # generate prediction DataFrame from prediction file
        poi_rank_df = pd.read_csv(fpredict, header=None)
        poi_rank_df.rename(columns={0: 'rank'}, inplace=True)
        poi_rank_df['poiID'] = test_df['poiID'].astype(np.int)
        poi_rank_df.set_index('poiID', inplace=True)
        poi_rank_df['probability'] = softmax(poi_rank_df['rank'])

        # remove test file and prediction file
        if self.debug == False:
            os.unlink(ftest)
            os.unlink(ftest_scaled)
            os.unlink(fpredict)

        return poi_rank_df


def gen_train_subdf(poi_id, query_id_set, poi_info, poi_clusters, cats, clusters, query_id_rdict):
    assert (isinstance(cats, list))
    assert (isinstance(clusters, list))

    columns = DF_COLUMNS
    poi_distmat = POI_DISTMAT
    df_ = pd.DataFrame(index=np.arange(len(query_id_set)), columns=columns)

    pop, nvisit = poi_info.loc[poi_id, 'popularity'], poi_info.loc[poi_id, 'nVisit']
    cat, cluster = poi_info.loc[poi_id, 'poiCat'], poi_clusters.loc[poi_id, 'clusterID']
    duration = poi_info.loc[poi_id, 'avgDuration']

    for j in range(len(query_id_set)):
        qid = query_id_set[j]
        assert (qid in query_id_rdict)  # qid --> (start, end, length)
        (p0, pN, trajLen) = query_id_rdict[qid]
        idx = df_.index[j]
        df_.loc[idx, 'poiID'] = poi_id
        df_.loc[idx, 'queryID'] = qid
        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))
        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))
        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)
        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)
        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)
        df_.loc[idx, 'trajLen'] = trajLen
        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiCat'] else -1
        df_.loc[idx, 'sameCatEnd'] = 1 if cat == poi_info.loc[pN, 'poiCat'] else -1
        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi_id, p0]
        df_.loc[idx, 'distEnd'] = poi_distmat.loc[poi_id, pN]
        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']
        df_.loc[idx, 'diffPopEnd'] = pop - poi_info.loc[pN, 'popularity']
        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']
        df_.loc[idx, 'diffNVisitEnd'] = nvisit - poi_info.loc[pN, 'nVisit']
        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']
        df_.loc[idx, 'diffDurationEnd'] = duration - poi_info.loc[pN, 'avgDuration']
        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1
        df_.loc[idx, 'sameNeighbourhoodEnd'] = 1 if cluster == poi_clusters.loc[pN, 'clusterID'] else -1

    return df_
def calc_dist_vec(longitudes1, latitudes1, longitudes2, latitudes2):
    """Calculate the distance (unit: km) between two places on earth, vectorised"""
    # convert degrees to radians
    lng1 = np.radians(longitudes1)
    lat1 = np.radians(latitudes1)
    lng2 = np.radians(longitudes2)
    lat2 = np.radians(latitudes2)
    radius = 6371.0088 # mean earth radius, en.wikipedia.org/wiki/Earth_radius#Mean_radius

    # The haversine formula, en.wikipedia.org/wiki/Great-circle_distance
    dlng = np.fabs(lng1 - lng2)
    dlat = np.fabs(lat1 - lat2)
    dist =  2 * radius * np.arcsin( np.sqrt(
                (np.sin(0.5*dlat))**2 + np.cos(lat1) * np.cos(lat2) * (np.sin(0.5*dlng))**2 ))
    return dist

POI_DISTMAT = pd.DataFrame(data=np.zeros((poi_all.shape[0], poi_all.shape[0]), dtype=np.float),
                           index=poi_all.index, columns=poi_all.index)

for ix in poi_all.index:
    POI_DISTMAT.loc[ix] = calc_dist_vec(poi_all.loc[ix, 'poiLon'],
                                        poi_all.loc[ix, 'poiLat'],
                                        poi_all['poiLon'],
                                        poi_all['poiLat'])
DF_COLUMNS = ['poiID', 'label', 'queryID', 'category', 'neighbourhood', 'popularity', 'nVisit', 'avgDuration',
              'trajLen', 'sameCatStart', 'sameCatEnd', 'distStart', 'distEnd', 'diffPopStart', 'diffPopEnd',
              'diffNVisitStart', 'diffNVisitEnd', 'diffDurationStart', 'diffDurationEnd',
              'sameNeighbourhoodStart', 'sameNeighbourhoodEnd']
def gen_train_df(trajid_list, traj_dict, poi_info, poi_clusters, cats, clusters, n_jobs=-1):
    columns = DF_COLUMNS
    poi_distmat = POI_DISTMAT
    query_id_dict = QUERY_ID_DICT
    train_trajs = [traj_dict[x] for x in trajid_list if len(traj_dict[x]) > 2]

    qid_set = sorted(set([query_id_dict[(t[0], t[-1], len(t))] for t in train_trajs]))
    poi_set = set()
    for tr in train_trajs:
        poi_set = poi_set | set(tr)

    query_id_rdict = dict()
    for k, v in query_id_dict.items():
        query_id_rdict[v] = k  # qid --> (start, end, length)

    train_df_list = Parallel(n_jobs=n_jobs)\
        (delayed(gen_train_subdf)(poi, qid_set, poi_info, poi_clusters, cats, clusters, query_id_rdict)
         for poi in poi_set)

    assert (len(train_df_list) > 0)
    df_ = train_df_list[0]
    for j in range(1, len(train_df_list)):
        df_ = df_.append(train_df_list[j], ignore_index=True)

        # set label
    df_.set_index(['queryID', 'poiID'], inplace=True)
    df_['label'] = 0
    for t in train_trajs:
        qid = query_id_dict[(t[0], t[-1], len(t))]
        for poi in t[1:-1]:  # do NOT count if the POI is startPOI/endPOI
            df_.loc[(qid, poi), 'label'] += 1

    df_.reset_index(inplace=True)
    return df_

def calc_poi_info(trajid_list, traj_all, poi_all):
    assert (len(trajid_list) > 0)
    poi_info = traj_all[traj_all['trajID'] == trajid_list[0]][['poiID', 'poiDuration']].copy()
    for i in range(1, len(trajid_list)):
        traj = traj_all[traj_all['trajID'] == trajid_list[i]][['poiID', 'poiDuration']]
        poi_info = poi_info.append(traj, ignore_index=True)

    poi_info = poi_info.groupby('poiID').agg([np.mean, np.size])
    poi_info.columns = poi_info.columns.droplevel()
    poi_info.reset_index(inplace=True)
    poi_info.rename(columns={'mean': 'avgDuration', 'size': 'nVisit'}, inplace=True)
    poi_info.set_index('poiID', inplace=True)
    poi_info['poiCat'] = poi_all.loc[poi_info.index, 'poiCat']
    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']
    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']

    # POI popularity: the number of distinct users that visited the POI
    pop_df = traj_all[traj_all['trajID'].isin(trajid_list)][['poiID', 'userID']].copy()
    pop_df = pop_df.groupby('poiID').agg(pd.Series.nunique)
    pop_df.rename(columns={'userID': 'nunique'}, inplace=True)
    poi_info['popularity'] = pop_df.loc[poi_info.index, 'nunique']

    return poi_info.copy()


def extract_traj(tid, traj_all):
    return traj_all[tid]
def maketrajall(file):
    trajall = open(file)
    id = 0
    traj_all = {}
    trajid_set_all = []
    for line in trajall:
        traj_all[id] = line
        trajid_set_all.append(id)
        id += 1
    trajall.close()
    return traj_all, trajid_set_all

poi_info_all = calc_poi_info(trajid_set_all, traj_all, poi_all)
poi_train = sorted(poi_info_all.index)
X = poi_all.loc[poi_train, ['poiLon', 'poiLat']]
nclusters = BIN_CLUSTER
kmeans = KMeans(n_clusters=nclusters, random_state=987654321)
kmeans.fit(X)
clusters = kmeans.predict(X)
POI_CLUSTER_LIST = sorted(np.unique(clusters))
POI_CLUSTERS = pd.DataFrame(data=clusters, index=poi_train)
POI_CLUSTERS.index.name = 'poiID'
POI_CLUSTERS.rename(columns={0:'clusterID'}, inplace=True)
POI_CLUSTERS['clusterID'] = POI_CLUSTERS['clusterID'].astype(np.int)
diff = poi_all.loc[poi_train, ['poiLon', 'poiLat']].max() - poi_all.loc[poi_train, ['poiLon', 'poiLat']].min()
ratio = diff['poiLon'] / diff['poiLat']
height = 6; width = int(round(ratio)*height)


def POIandPOP(trajid_set_all,traj_all):
    recdict_rank = dict()
    traj_dict = dict()
    for trajid in trajid_set_all:
        traj = extract_traj(trajid, traj_all)
        assert (trajid not in traj_dict)
        traj_dict[trajid] = traj
    cnt = 1
    for i in range(len(trajid_set_all)):
        tid = trajid_set_all[i]
        te = traj_dict[tid]

        # trajectory is too short
        if len(te) < 3: continue

        trajid_list_train = trajid_set_all[:i] + trajid_set_all[i + 1:]

        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)

        # start/end is not in training set
        if not (te[0] in poi_info.index and te[-1] in poi_info.index): continue

        print(te, '#%d ->' % cnt);
        cnt += 1;
        sys.stdout.flush()

        # recommendation leveraging ranking
        train_df = gen_train_df(trajid_list_train, traj_dict, poi_info, poi_clusters=POI_CLUSTERS,cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST, n_jobs=N_JOBS)
        ranksvm = RankSVM(ranksvm_dir, useLinear=True)
        ranksvm.train(train_df, cost=RANKSVM_COST)
        test_df = gen_test_df(te[0], te[-1], len(te), poi_info, poi_clusters=POI_CLUSTERS,cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)
        rank_df = ranksvm.predict(test_df)

        # POI popularity based ranking
        poi_info.sort_values(by='popularity', ascending=False, inplace=True)
        ranks1 = poi_info.index.tolist()
        rank_pop = [te[0]] + [x for x in ranks1 if x not in {te[0], te[-1]}][:len(te) - 2] + [te[-1]]

        # POI feature based ranking
        rank_df.sort_values(by='rank', ascending=False, inplace=True)
        ranks2 = rank_df.index.tolist()
        rank_feature = [te[0]] + [x for x in ranks2 if x not in {te[0], te[-1]}][:len(te) - 2] + [te[-1]]

        recdict_rank[tid] = {'REAL': te, 'REC_POP': rank_pop, 'REC_FEATURE': rank_feature}
        print(' ' * 10, 'Rank POP:', rank_pop);
        print(' ' * 10, 'Rank POI:', rank_feature);
        sys.stdout.flush()